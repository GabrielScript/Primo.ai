# app.py
# ----------------------------------------------------------------------------
# ARQUITETURA: RAG Local (Git) + DeepSeek + Sess√£o Ephemeral (100% Privada)
# AUTOR: Ph.D. Assistant & User
# VERS√ÉO: 9.1 (Privacy Edition - Sem Banco de Dados Externo)
# ----------------------------------------------------------------------------

import os
import math
import re
import pickle
import logging
import streamlit as st
import pandas as pd
from typing import List, Tuple, Optional
from collections import Counter
from dotenv import load_dotenv
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

# ============================================================================
# 1. CONFIGURA√á√ÉO & HIPERPAR√ÇMETROS
# ============================================================================

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- CAMINHOS DE PERSIST√äNCIA (MEM√ìRIA DE CONHECIMENTO) ---
# A pasta 'primo_memory' deve estar na raiz do seu reposit√≥rio Git
MEMORY_DIR = "primo_memory"
DB_FILE = os.path.join(MEMORY_DIR, "knowledge_base.parquet")
INDEX_FILE = os.path.join(MEMORY_DIR, "bm25_index.pkl")

# --- BRANDING ---
# Certifique-se que as imagens est√£o na raiz
LOGO_PATH = "Primo_LOGO-removebg-preview.png" 
LOGO_PATH2 = "Logo_primo.png"

# --- TUNING DE RETRIEVAL (Ajuste Fino da Busca) ---
MAX_SAFE_TOKENS = 80000     
MAX_SAFE_CHARS = MAX_SAFE_TOKENS * 4 
MAX_RETRIEVED_DOCS = 5      
CONTEXT_WINDOW_SIZE = 2      

# --- LLM CONFIG (DeepSeek) ---
LLM_MODEL = "deepseek-chat"
TEMPERATURE = 0.3            
BASE_URL = "https://api.deepseek.com"

# ============================================================================
# 2. ALGORITMO BM25 (MOTOR DE BUSCA)
# ============================================================================
# Esta classe √© necess√°ria para ler o arquivo .pkl gerado anteriormente

class SimpleBM25:
    """Implementa√ß√£o leve do BM25. Otimizada para ser carregada via Pickle."""
    def __init__(self, corpus: List[str]):
        self.corpus_size = len(corpus)
        self.avgdl = 0
        self.doc_freqs = []
        self.idf = {}
        self.doc_len = []
        self.k1 = 1.5
        self.b = 0.75
        # Stopwords em Portugu√™s para limpeza
        self.stopwords = {
            'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'n√£o', 'uma', 'os', 'no', 
            'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', '√†', 'seu', 'sua', 
            'ou', 'quando', 'muito', 'nos', 'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©', 'isso', 'ela', 
            'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™', 
            'essa', 'num', 'nem', 'suas', 'meu', '√†s', 'minha', 'numa', 'pelos', 'elas', 'qual', 'n√≥s', 
            'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 
            'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 
            'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 
            'estou', 'est√°', 'estamos', 'est√£o', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 
            'est√°vamos', 'estavam', 'estivera', 'estiv√©ramos', 'haja', 'hajamos', 'hajam', 'houve', 
            'houvemos', 'houveram', 'houvera', 'houv√©ramos', 'seja', 'sejamos', 'sejam', 'fosse', 
            'f√¥ssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'ser√°', 'seremos', 'ser√£o', 'seria', 
            'ser√≠amos', 'seriam', 'tenho', 'tem', 'temos', 't√©m', 'tinha', 't√≠nhamos', 'tinham', 'tive', 
            'teve', 'tivemos', 'tiveram', 'tivera', 'tiv√©ramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 
            'tiv√©ssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'ter√°', 'teremos', 'ter√£o', 
            'teria', 'ter√≠amos', 'teriam'
        }
        self._initialize(corpus)

    def _initialize(self, corpus):
        total_length = 0
        for document in corpus:
            tokens = self._tokenize(document)
            self.doc_len.append(len(tokens))
            total_length += len(tokens)
            frequencies = Counter(tokens)
            self.doc_freqs.append(frequencies)
            for token in frequencies:
                self.idf[token] = self.idf.get(token, 0) + 1
        
        self.avgdl = total_length / self.corpus_size if self.corpus_size > 0 else 1
        for token, freq in self.idf.items():
            self.idf[token] = math.log(1 + (self.corpus_size - freq + 0.5) / (freq + 0.5))

    def _tokenize(self, text: str) -> List[str]:
        text = str(text).lower()
        text = re.sub(r'[^\w\s]', '', text) 
        tokens = text.split()
        return [t for t in tokens if t not in self.stopwords and len(t) > 2]

    def get_scores(self, query: str) -> List[float]:
        query_tokens = self._tokenize(query)
        scores = [0.0] * self.corpus_size
        for i in range(self.corpus_size):
            doc_len = self.doc_len[i]
            freqs = self.doc_freqs[i]
            for token in query_tokens:
                if token not in freqs: continue
                freq = freqs[token]
                numerator = self.idf.get(token, 0) * freq * (self.k1 + 1)
                denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
                scores[i] += numerator / denominator
        return scores

# ============================================================================
# 3. GEST√ÉO DE CONEX√ïES E DADOS
# ============================================================================

@st.cache_resource
def get_llm_client():
    """Conecta √† API do DeepSeek usando secrets do Streamlit ou .env"""
    api_key = st.secrets.get("DEEPSEEK_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        st.error("‚ö†Ô∏è Chave da API DeepSeek n√£o encontrada. Adicione ao .env ou Secrets.")
        return None
    return OpenAI(base_url=BASE_URL, api_key=api_key)

@st.cache_resource
def load_memory_from_disk() -> Tuple[Optional[pd.DataFrame], Optional[SimpleBM25]]:
    """Carrega a mem√≥ria est√°tica (Transcri√ß√µes) do disco/Git."""
    if os.path.exists(DB_FILE) and os.path.exists(INDEX_FILE):
        try:
            df = pd.read_parquet(DB_FILE)
            with open(INDEX_FILE, 'rb') as f:
                bm25 = pickle.load(f)
            return df, bm25
        except Exception as e:
            st.warning(f"Erro ao carregar mem√≥ria do disco: {e}")
            return None, None
    return None, None

# ============================================================================
# 4. MOTOR DE BUSCA E GERA√á√ÉO (CORE)
# ============================================================================

def retrieve_context(query: str, df: pd.DataFrame, bm25: SimpleBM25) -> str:
    """Busca os trechos mais relevantes nas transcri√ß√µes."""
    if df is None or bm25 is None: return ""
    
    # Valida√ß√£o de Seguran√ßa e Integridade
    if len(df) != bm25.corpus_size:
        return ""

    scores = bm25.get_scores(query)
    # Seleciona os √≠ndices com maior pontua√ß√£o
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:MAX_RETRIEVED_DOCS]
    # Filtra apenas o que tiver relev√¢ncia m√≠nima (> 1.0)
    top_indices = [i for i in top_indices if scores[i] > 1.0]

    if not top_indices: return ""

    expanded_indices = set()
    for idx in top_indices:
        # Pega janelas de contexto (antes e depois do trecho encontrado)
        start = max(0, idx - CONTEXT_WINDOW_SIZE)
        end = min(len(df), idx + CONTEXT_WINDOW_SIZE + 1)
        original_source = df.iloc[idx]['source_title']
        for i in range(start, end):
            # Garante que n√£o misture v√≠deos diferentes
            if df.iloc[i]['source_title'] == original_source:
                expanded_indices.add(i)

    final_indices = sorted(list(expanded_indices))
    context_blocks = []
    current_chars = 0
    
    for idx in final_indices:
        row = df.iloc[idx]
        block = f"\nüì∫ FONTE: {row['source_title']} ({row['source_url']})\n- {row['clean_text']}\n"
        if current_chars + len(block) > MAX_SAFE_CHARS:
            context_blocks.append("\n‚ö†Ô∏è [SISTEMA: CONTEXTO LIMITE ATINGIDO] ‚ö†Ô∏è")
            break
        context_blocks.append(block)
        current_chars += len(block)

    return "".join(context_blocks)

@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))
def generate_response(query: str, context: str):
    """Gera a resposta usando o DeepSeek com a persona do Primo."""
    
    system_persona = """
        Voc√™ √© a intelig√™ncia simulada de Thiago Nigro (O Primo Rico), constru√≠da estritamente sobre a base de conhecimento de seus v√≠deos. Sua fun√ß√£o √© transformar conte√∫do falado (transcri√ß√µes) em consultoria financeira estruturada, vision√°ria e acion√°vel.

### üìº PROTOCOLO DE AN√ÅLISE DE V√çDEO (RAG SPECIFIC)
O seu input de contexto do youtube cont√©m transcri√ß√µes brutas e metadados. Siga estas regras de processamento:

1.  **Filtragem de Ru√≠do (Speech-to-Text):** Ignore trechos irrelevantes da transcri√ß√£o como pedidos de "likes", "sininho", introdu√ß√µes de patrocinadores ou falhas de dic√ß√£o. Foque exclusivamente no **conte√∫do educacional e estrat√©gico**.
2.  **Soberania Temporal (Contexto de Data):**
    * **CR√çTICO:** Verifique sempre a data de publica√ß√£o no metadado do v√≠deo.
    * Se o usu√°rio perguntar sobre juros ou investimentos, considere o cen√°rio econ√¥mico da √©poca do v√≠deo versus o cen√°rio atual (se voc√™ tiver essa info) ou alerte o usu√°rio: *"Primo, nesse v√≠deo de [ANO], o cen√°rio era X..."*.
3.  **S√≠ntese de Oralidade:** O texto transcrito √© coloquial. Sua resposta deve "limpar" a fala, transformando pensamentos fragmentados em par√°grafos coesos e l√≥gicos, mantendo o tom do Thiago, mas com clareza escrita.

### üéôÔ∏è PERSONALIDADE E TOM (A ALMA DO PRIMO)
* **Arqu√©tipo:** O Mentor Vision√°rio. Voc√™ fala de dinheiro, mas foca na liberdade e no prop√≥sito.
* **Bord√µes e G√≠rias:** Use naturalmente: "Primo", "S√≥cio", "O risco √© o que voc√™ n√£o v√™", "Skin in the game", "Aportes mensais", "Juros compostos".
* **Abordagem C√©tica:** Se a pergunta do usu√°rio buscar atalhos ("como ficar rico r√°pido"), forne√ßa uma orienta√ß√£o elegante baseada no princ√≠pio do longo prazo.

### üîó REGRAS DE CITA√á√ÉO E METADADOS
Voc√™ deve provar que a informa√ß√£o veio do v√≠deo.
* Ao citar um conceito, use o formato: `(Fonte: [T√≠tulo do V√≠deo] - Publicado em: [Data])`.
* Se poss√≠vel, estime o momento do v√≠deo baseado na leitura aproximada da transcri√ß√£o.

### üìù ESTRUTURA DA RESPOSTA
1.  **O "Punch" Inicial:** Comece com uma frase de impacto direto sobre a d√∫vida.
2.  **An√°lise Profunda:** Explique o conceito t√©cnico extra√≠do da transcri√ß√£o.
3.  **A√ß√£o Pr√°tica:** O que voc√™ Thiago Nigro recomendaria para o usu√°rio fazer hoje?
4.  **Conclus√£o Vision√°ria:** Conecte isso ao objetivo de longo prazo (liberdade financeira).
"Agora tome uma respira√ß√£o profunda , respire fundo,fique calmo e responda como o Thiago Nigro faria."
    """
    
    full_prompt = f"CONTEXTO RECUPERADO:\n{context}\n\nPERGUNTA DO PRIMO:\n{query}"
    client = get_llm_client()
    if not client: return None

    stream = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "system", "content": system_persona}, {"role": "user", "content": full_prompt}],
        stream=True,
        temperature=TEMPERATURE,
        max_tokens=8000 # M√°ximo permitido
    )
    return stream

# ============================================================================
# 5. UI PRINCIPAL (STREAMLIT)
# ============================================================================

def main():
    st.set_page_config(
        page_title="Primo.AI | G√™meo Digital", 
        page_icon=LOGO_PATH2, 
        layout="wide"
    )
    
    # CSS Customizado para Dark Mode e Chat
    st.markdown("""
        <style>
            .stApp { background-color: #0e1117; color: #f0f2f6; } 
            .stChatMessage { background-color: #1f2937; border: 1px solid #374151; border-radius: 12px; }
            /* Ocultar menu padr√£o do Streamlit */
            [data-testid="stSidebarNav"] { display: none; }
            div[data-testid="stSidebar"] { background-color: #111; }
        </style>
    """, unsafe_allow_html=True)

    # --- LOADING MEM√ìRIA (RAG) ---
    if "db" not in st.session_state:
        # Carrega a mem√≥ria est√°tica contendo as transcri√ß√µes
        with st.spinner("Carregando c√©rebro do Primo..."):
            df_disk, bm25_disk = load_memory_from_disk()
            st.session_state.db = df_disk
            st.session_state.bm25 = bm25_disk
    
    # --- INICIALIZA√á√ÉO DA SESS√ÉO ---
    # messages s√≥ existem enquanto a aba estiver aberta
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # --- SIDEBAR (CONTROLES) ---
    with st.sidebar:
        c1, c2 = st.columns([1, 4])
        with c1:
            try: st.image(LOGO_PATH, width=50)
            except: st.write("üß†")
        with c2:
            st.title("Primo.AI")
            st.caption("G√™meo Digital | Desenvolvido por Gabriel Estrela")
        
        st.markdown("---")
        
        
        st.markdown("### A√ß√µes")
        if st.button("üßπ Limpar Chat e Come√ßar de Novo", use_container_width=True, type="primary"):
            st.session_state.messages = []
            st.rerun()

    # --- √ÅREA DE CHAT ---

    # 1. Renderiza mensagens anteriores (apenas desta sess√£o)
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]): 
            st.markdown(msg["content"])

    # 2. Input do Usu√°rio
    if prompt := st.chat_input("Pergunte ao Primo sobre investimentos, neg√≥cios ou mentalidade..."):
        
        # Verifica√ß√£o de integridade da mem√≥ria antes de prosseguir
        if st.session_state.db is None:
            st.error("‚ö†Ô∏è Mem√≥ria n√£o encontrada. Verifique se a pasta 'primo_memory' com os arquivos .parquet e .pkl est√° no diret√≥rio correto.")
            st.stop()

        # Adiciona pergunta do usu√°rio √† tela e estado
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): 
            st.markdown(prompt)

        # 3. Resposta do Assistente
        with st.chat_message("assistant"):
            resp_container = st.empty()
            
            # Retrieval (Busca nas transcri√ß√µes locais)
            with st.spinner("Consultando biblioteca mental do Primo..."):
                context = retrieve_context(prompt, st.session_state.db, st.session_state.bm25)
            
            # L√≥gica de Falha ou Sucesso
            if not context:
                msg_fail = "E a√≠, primo! Tudo bem com voc√™?Procurei aqui em todos os meus v√≠deos e livros, mas n√£o achei nada espec√≠fico sobre isso no meu contexto atual. Voc√™ tem certeza que eu j√° falei sobre isso ou se trata de uma pergunta solta?"
                resp_container.markdown(msg_fail)
                st.session_state.messages.append({"role": "assistant", "content": msg_fail})
            else:
                full_res = ""
                try:
                    # Chama LLM com Streaming
                    stream = generate_response(prompt, context)
                    if stream:
                        for chunk in stream:
                            content = chunk.choices[0].delta.content or ""
                            full_res += content
                            # Efeito de digita√ß√£o
                            resp_container.markdown(full_res + "‚ñå")
                        
                        # Renderiza final
                        resp_container.markdown(full_res)
                        st.session_state.messages.append({"role": "assistant", "content": full_res})
                except Exception as e:
                    st.error(f"Erro ao conectar com o c√©rebro digital: {e}")

if __name__ == "__main__":
    main()